{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35350d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\anaconda3\\envs\\Viver\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import functions as f\n",
    "\n",
    "from Text2 import *\n",
    "from LSTM_class import *\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "import tokenizer_vars as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a43376",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_recipes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39fd4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train, path_test = f'data/train_{number_of_recipes}.txt', f'data/test_{number_of_recipes}.txt'\n",
    "\n",
    "input_train = f.read_txt(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5635db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tv.getTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e2172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 76294, distinct tokens: 2094\n",
      "number of sequences of length 4: 25430\n"
     ]
    }
   ],
   "source": [
    "max_len = 4\n",
    "step = 3\n",
    "\n",
    "token2ind = tokenizer.get_vocab()\n",
    "ind2token = {ind: token for token, ind in token2ind.items()}\n",
    "\n",
    "text_train = Text2(input_train, token2ind=token2ind, ind2token=ind2token)\n",
    "text_train.tokens_info()\n",
    "\n",
    "seq_train = Sequences(text_train, max_len, step)\n",
    "seq_train.sequences_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd2381a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '¤',\n",
       " '¥',\n",
       " '¦',\n",
       " '§',\n",
       " '¨',\n",
       " '©',\n",
       " 'ª',\n",
       " '«',\n",
       " '¬',\n",
       " '®',\n",
       " '¯',\n",
       " '°',\n",
       " '±',\n",
       " '²',\n",
       " '³',\n",
       " '´',\n",
       " 'µ',\n",
       " '¶',\n",
       " '·',\n",
       " '¸',\n",
       " '¹',\n",
       " 'º',\n",
       " '»',\n",
       " '¼',\n",
       " '½',\n",
       " '¾',\n",
       " '¿',\n",
       " 'À',\n",
       " 'Á',\n",
       " 'Â',\n",
       " 'Ã',\n",
       " 'Ä',\n",
       " 'Å',\n",
       " 'Æ',\n",
       " 'Ç',\n",
       " 'È',\n",
       " 'É',\n",
       " 'Ê',\n",
       " 'Ë',\n",
       " 'Ì',\n",
       " 'Í',\n",
       " 'Î',\n",
       " 'Ï',\n",
       " 'Ð',\n",
       " 'Ñ',\n",
       " 'Ò',\n",
       " 'Ó',\n",
       " 'Ô',\n",
       " 'Õ',\n",
       " 'Ö',\n",
       " '×',\n",
       " 'Ø',\n",
       " 'Ù',\n",
       " 'Ú',\n",
       " 'Û',\n",
       " 'Ü',\n",
       " 'Ý',\n",
       " 'Þ',\n",
       " 'ß',\n",
       " 'à',\n",
       " 'á',\n",
       " 'â',\n",
       " 'ã',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'ì',\n",
       " 'í',\n",
       " 'î',\n",
       " 'ï',\n",
       " 'ð',\n",
       " 'ñ',\n",
       " 'ò',\n",
       " 'ó',\n",
       " 'ô',\n",
       " 'õ',\n",
       " 'ö',\n",
       " '÷',\n",
       " 'ø',\n",
       " 'ù',\n",
       " 'ú',\n",
       " 'û',\n",
       " 'ü',\n",
       " 'ý',\n",
       " 'þ',\n",
       " 'ÿ',\n",
       " 'Ā',\n",
       " 'ā',\n",
       " 'Ă',\n",
       " 'ă',\n",
       " 'Ą',\n",
       " 'ą',\n",
       " 'Ć',\n",
       " 'ć',\n",
       " 'Ĉ',\n",
       " 'ĉ',\n",
       " 'Ċ',\n",
       " 'ċ',\n",
       " 'Č',\n",
       " 'č',\n",
       " 'Ď',\n",
       " 'ď',\n",
       " 'Đ',\n",
       " 'đ',\n",
       " 'Ē',\n",
       " 'ē',\n",
       " 'Ĕ',\n",
       " 'ĕ',\n",
       " 'Ė',\n",
       " 'ė',\n",
       " 'Ę',\n",
       " 'ę',\n",
       " 'Ě',\n",
       " 'ě',\n",
       " 'Ĝ',\n",
       " 'ĝ',\n",
       " 'Ğ',\n",
       " 'ğ',\n",
       " 'Ġ',\n",
       " 'ġ',\n",
       " 'Ģ',\n",
       " 'ģ',\n",
       " 'Ĥ',\n",
       " 'ĥ',\n",
       " 'Ħ',\n",
       " 'ħ',\n",
       " 'Ĩ',\n",
       " 'ĩ',\n",
       " 'Ī',\n",
       " 'ī',\n",
       " 'Ĭ',\n",
       " 'ĭ',\n",
       " 'Į',\n",
       " 'į',\n",
       " 'İ',\n",
       " 'ı',\n",
       " 'Ĳ',\n",
       " 'ĳ',\n",
       " 'Ĵ',\n",
       " 'ĵ',\n",
       " 'Ķ',\n",
       " 'ķ',\n",
       " 'ĸ',\n",
       " 'Ĺ',\n",
       " 'ĺ',\n",
       " 'Ļ',\n",
       " 'ļ',\n",
       " 'Ľ',\n",
       " 'ľ',\n",
       " 'Ŀ',\n",
       " 'ŀ',\n",
       " 'Ł',\n",
       " 'ł',\n",
       " 'Ń',\n",
       " 'Ġt',\n",
       " 'Ġa',\n",
       " 'he',\n",
       " 'in',\n",
       " 're',\n",
       " 'on',\n",
       " 'Ġthe',\n",
       " 'er',\n",
       " 'Ġs',\n",
       " 'at',\n",
       " 'Ġw',\n",
       " 'Ġo',\n",
       " 'en',\n",
       " 'Ġc',\n",
       " 'it',\n",
       " 'is',\n",
       " 'an',\n",
       " 'or',\n",
       " 'es',\n",
       " 'Ġb',\n",
       " 'ed',\n",
       " 'Ġf',\n",
       " 'ing',\n",
       " 'Ġp',\n",
       " 'ou',\n",
       " 'Ġan',\n",
       " 'al',\n",
       " 'ar',\n",
       " 'Ġto',\n",
       " 'Ġm',\n",
       " 'Ġof',\n",
       " 'Ġin',\n",
       " 'Ġd',\n",
       " 'Ġh',\n",
       " 'Ġand',\n",
       " 'ic',\n",
       " 'as',\n",
       " 'le',\n",
       " 'Ġth',\n",
       " 'ion',\n",
       " 'om',\n",
       " 'll',\n",
       " 'ent',\n",
       " 'Ġn',\n",
       " 'Ġl',\n",
       " 'st',\n",
       " 'Ġre',\n",
       " 've',\n",
       " 'Ġe',\n",
       " 'ro',\n",
       " 'ly',\n",
       " 'Ġbe',\n",
       " 'Ġg',\n",
       " 'ĠT',\n",
       " 'ct',\n",
       " 'ĠS',\n",
       " 'id',\n",
       " 'ot',\n",
       " 'ĠI',\n",
       " 'ut',\n",
       " 'et',\n",
       " 'ĠA',\n",
       " 'Ġis',\n",
       " 'Ġon',\n",
       " 'im',\n",
       " 'am',\n",
       " 'ow',\n",
       " 'ay',\n",
       " 'ad',\n",
       " 'se',\n",
       " 'Ġthat',\n",
       " 'ĠC',\n",
       " 'ig',\n",
       " 'Ġfor',\n",
       " 'ac',\n",
       " 'Ġy',\n",
       " 'ver',\n",
       " 'ur',\n",
       " 'Ġu',\n",
       " 'ld',\n",
       " 'Ġst',\n",
       " 'ĠM',\n",
       " \"'s\",\n",
       " 'Ġhe',\n",
       " 'Ġit',\n",
       " 'ation',\n",
       " 'ith',\n",
       " 'ir',\n",
       " 'ce',\n",
       " 'Ġyou',\n",
       " 'il',\n",
       " 'ĠB',\n",
       " 'Ġwh',\n",
       " 'ol',\n",
       " 'ĠP',\n",
       " 'Ġwith',\n",
       " 'Ġ1',\n",
       " 'ter',\n",
       " 'ch',\n",
       " 'Ġas',\n",
       " 'Ġwe',\n",
       " 'Ġ(',\n",
       " 'nd',\n",
       " 'ill',\n",
       " 'ĠD',\n",
       " 'if',\n",
       " 'Ġ2',\n",
       " 'ag',\n",
       " 'ers',\n",
       " 'ke',\n",
       " 'Ġ\"',\n",
       " 'ĠH',\n",
       " 'em',\n",
       " 'Ġcon',\n",
       " 'ĠW',\n",
       " 'ĠR',\n",
       " 'her',\n",
       " 'Ġwas',\n",
       " 'Ġr',\n",
       " 'od',\n",
       " 'ĠF',\n",
       " 'ul',\n",
       " 'ate',\n",
       " 'Ġat',\n",
       " 'ri',\n",
       " 'pp',\n",
       " 'ore',\n",
       " 'ĠThe',\n",
       " 'Ġse',\n",
       " 'us',\n",
       " 'Ġpro',\n",
       " 'Ġha',\n",
       " 'um',\n",
       " 'Ġare',\n",
       " 'Ġde',\n",
       " 'ain',\n",
       " 'and',\n",
       " 'Ġor',\n",
       " 'igh',\n",
       " 'est',\n",
       " 'ist',\n",
       " 'ab',\n",
       " 'rom',\n",
       " 'ĠN',\n",
       " 'th',\n",
       " 'Ġcom',\n",
       " 'ĠG',\n",
       " 'un',\n",
       " 'op',\n",
       " '00',\n",
       " 'ĠL',\n",
       " 'Ġnot',\n",
       " 'ess',\n",
       " 'Ġex',\n",
       " 'Ġv',\n",
       " 'res',\n",
       " 'ĠE',\n",
       " 'ew',\n",
       " 'ity',\n",
       " 'ant',\n",
       " 'Ġby',\n",
       " 'el',\n",
       " 'os',\n",
       " 'ort',\n",
       " 'oc',\n",
       " 'qu',\n",
       " 'Ġfrom',\n",
       " 'Ġhave',\n",
       " 'Ġsu',\n",
       " 'ive',\n",
       " 'ould',\n",
       " 'Ġsh',\n",
       " 'Ġthis',\n",
       " 'nt',\n",
       " 'ra',\n",
       " 'pe',\n",
       " 'ight',\n",
       " 'art',\n",
       " 'ment',\n",
       " 'Ġal',\n",
       " 'ust',\n",
       " 'end',\n",
       " '--',\n",
       " 'all',\n",
       " 'ĠO',\n",
       " 'ack',\n",
       " 'Ġch',\n",
       " 'Ġle',\n",
       " 'ies',\n",
       " 'red',\n",
       " 'ard',\n",
       " 'âĢ',\n",
       " 'out',\n",
       " 'ĠJ',\n",
       " 'Ġab',\n",
       " 'ear',\n",
       " 'iv',\n",
       " 'ally',\n",
       " 'our',\n",
       " 'ost',\n",
       " 'gh',\n",
       " 'pt',\n",
       " 'Ġpl',\n",
       " 'ast',\n",
       " 'Ġcan',\n",
       " 'ak',\n",
       " 'ome',\n",
       " 'ud',\n",
       " 'The',\n",
       " 'Ġhis',\n",
       " 'Ġdo',\n",
       " 'Ġgo',\n",
       " 'Ġhas',\n",
       " 'ge',\n",
       " \"'t\",\n",
       " 'ĠU',\n",
       " 'rou',\n",
       " 'Ġsa',\n",
       " 'Ġj',\n",
       " 'Ġbut',\n",
       " 'Ġwor',\n",
       " 'Ġall',\n",
       " 'ect',\n",
       " 'Ġk',\n",
       " 'ame',\n",
       " 'Ġwill',\n",
       " 'ok',\n",
       " 'Ġwhe',\n",
       " 'Ġthey',\n",
       " 'ide',\n",
       " '01',\n",
       " 'ff',\n",
       " 'ich',\n",
       " 'pl',\n",
       " 'ther',\n",
       " 'Ġtr',\n",
       " '..',\n",
       " 'Ġint',\n",
       " 'ie',\n",
       " 'ure',\n",
       " 'age',\n",
       " 'Ġne',\n",
       " 'ial',\n",
       " 'ap',\n",
       " 'ine',\n",
       " 'ice',\n",
       " 'Ġme',\n",
       " 'Ġout',\n",
       " 'ans',\n",
       " 'one',\n",
       " 'ong',\n",
       " 'ions',\n",
       " 'Ġwho',\n",
       " 'ĠK',\n",
       " 'Ġup',\n",
       " 'Ġtheir',\n",
       " 'Ġad',\n",
       " 'Ġ3',\n",
       " 'Ġus',\n",
       " 'ated',\n",
       " 'ous',\n",
       " 'Ġmore',\n",
       " 'ue',\n",
       " 'og',\n",
       " 'ĠSt',\n",
       " 'ind',\n",
       " 'ike',\n",
       " 'Ġso',\n",
       " 'ime',\n",
       " 'per',\n",
       " '.\"',\n",
       " 'ber',\n",
       " 'iz',\n",
       " 'act',\n",
       " 'Ġone',\n",
       " 'Ġsaid',\n",
       " 'Ġ-',\n",
       " 'are',\n",
       " 'Ġyour',\n",
       " 'cc',\n",
       " 'ĠTh',\n",
       " 'Ġcl',\n",
       " 'ep',\n",
       " 'ake',\n",
       " 'able',\n",
       " 'ip',\n",
       " 'Ġcont',\n",
       " 'Ġwhich',\n",
       " 'ia',\n",
       " 'Ġim',\n",
       " 'Ġabout',\n",
       " 'Ġwere',\n",
       " 'very',\n",
       " 'ub',\n",
       " 'Ġhad',\n",
       " 'Ġen',\n",
       " 'Ġcomp',\n",
       " ',\"',\n",
       " 'ĠIn',\n",
       " 'Ġun',\n",
       " 'Ġag',\n",
       " 'ire',\n",
       " 'ace',\n",
       " 'au',\n",
       " 'ary',\n",
       " 'Ġwould',\n",
       " 'ass',\n",
       " 'ry',\n",
       " 'ĠâĢ',\n",
       " 'cl',\n",
       " 'ook',\n",
       " 'ere',\n",
       " 'so',\n",
       " 'ĠV',\n",
       " 'ign',\n",
       " 'ib',\n",
       " 'Ġoff',\n",
       " 'Ġte',\n",
       " 'ven',\n",
       " 'ĠY',\n",
       " 'ile',\n",
       " 'ose',\n",
       " 'ite',\n",
       " 'orm',\n",
       " 'Ġ201',\n",
       " 'Ġres',\n",
       " 'Ġman',\n",
       " 'Ġper',\n",
       " 'Ġother',\n",
       " 'ord',\n",
       " 'ult',\n",
       " 'Ġbeen',\n",
       " 'Ġlike',\n",
       " 'ase',\n",
       " 'ance',\n",
       " 'ks',\n",
       " 'ays',\n",
       " 'own',\n",
       " 'ence',\n",
       " 'Ġdis',\n",
       " 'ction',\n",
       " 'Ġany',\n",
       " 'Ġapp',\n",
       " 'Ġsp',\n",
       " 'int',\n",
       " 'ress',\n",
       " 'ations',\n",
       " 'ail',\n",
       " 'Ġ4',\n",
       " 'ical',\n",
       " 'Ġthem',\n",
       " 'Ġher',\n",
       " 'ount',\n",
       " 'ĠCh',\n",
       " 'Ġar',\n",
       " 'Ġif',\n",
       " 'Ġthere',\n",
       " 'Ġpe',\n",
       " 'Ġyear',\n",
       " 'av',\n",
       " 'Ġmy',\n",
       " 'Ġsome',\n",
       " 'Ġwhen',\n",
       " 'ough',\n",
       " 'ach',\n",
       " 'Ġthan',\n",
       " 'ru',\n",
       " 'ond',\n",
       " 'ick',\n",
       " 'Ġover',\n",
       " 'vel',\n",
       " 'Ġqu',\n",
       " 'ĊĊ',\n",
       " 'Ġsc',\n",
       " 'reat',\n",
       " 'ree',\n",
       " 'ĠIt',\n",
       " 'ound',\n",
       " 'port',\n",
       " 'Ġalso',\n",
       " 'Ġpart',\n",
       " 'fter',\n",
       " 'Ġkn',\n",
       " 'Ġbec',\n",
       " 'Ġtime',\n",
       " 'ens',\n",
       " 'Ġ5',\n",
       " 'ople',\n",
       " 'Ġwhat',\n",
       " 'Ġno',\n",
       " 'du',\n",
       " 'mer',\n",
       " 'ang',\n",
       " 'Ġnew',\n",
       " '----',\n",
       " 'Ġget',\n",
       " 'ory',\n",
       " 'ition',\n",
       " 'ings',\n",
       " 'Ġjust',\n",
       " 'Ġinto',\n",
       " 'Ġ0',\n",
       " 'ents',\n",
       " 'ove',\n",
       " 'te',\n",
       " 'Ġpeople',\n",
       " 'Ġpre',\n",
       " 'Ġits',\n",
       " 'Ġrec',\n",
       " 'Ġtw',\n",
       " 'ian',\n",
       " 'irst',\n",
       " 'ark',\n",
       " 'ors',\n",
       " 'Ġwork',\n",
       " 'ade',\n",
       " 'ob',\n",
       " 'Ġshe',\n",
       " 'Ġour',\n",
       " 'wn',\n",
       " 'ink',\n",
       " 'lic',\n",
       " 'Ġ19',\n",
       " 'ĠHe',\n",
       " 'ish',\n",
       " 'nder',\n",
       " 'ause',\n",
       " 'Ġhim',\n",
       " 'ons',\n",
       " 'Ġ[',\n",
       " 'Ġro',\n",
       " 'form',\n",
       " 'ild',\n",
       " 'ates',\n",
       " 'vers',\n",
       " 'Ġonly',\n",
       " 'oll',\n",
       " 'Ġspe',\n",
       " 'ck',\n",
       " 'ell',\n",
       " 'amp',\n",
       " 'Ġacc',\n",
       " 'Ġbl',\n",
       " 'ious',\n",
       " 'urn',\n",
       " 'ft',\n",
       " 'ood',\n",
       " 'Ġhow',\n",
       " 'hed',\n",
       " \"Ġ'\",\n",
       " 'Ġafter',\n",
       " 'aw',\n",
       " 'Ġatt',\n",
       " 'ov',\n",
       " 'ne',\n",
       " 'Ġplay',\n",
       " 'erv',\n",
       " 'ict',\n",
       " 'Ġcould',\n",
       " 'itt',\n",
       " 'Ġam',\n",
       " 'Ġfirst',\n",
       " 'Ġ6',\n",
       " 'Ġact',\n",
       " 'Ġ$',\n",
       " 'ec',\n",
       " 'hing',\n",
       " 'ual',\n",
       " 'ull',\n",
       " 'Ġcomm',\n",
       " 'oy',\n",
       " 'old',\n",
       " 'ces',\n",
       " 'ater',\n",
       " 'Ġfe',\n",
       " 'Ġbet',\n",
       " 'we',\n",
       " 'iff',\n",
       " 'Ġtwo',\n",
       " 'ock',\n",
       " 'Ġback',\n",
       " ').',\n",
       " 'ident',\n",
       " 'Ġunder',\n",
       " 'rough',\n",
       " 'sel',\n",
       " 'xt',\n",
       " 'Ġmay',\n",
       " 'round',\n",
       " 'Ġpo',\n",
       " 'ph',\n",
       " 'iss',\n",
       " 'Ġdes',\n",
       " 'Ġmost',\n",
       " 'Ġdid',\n",
       " 'Ġadd',\n",
       " 'ject',\n",
       " 'Ġinc',\n",
       " 'fore',\n",
       " 'Ġpol',\n",
       " 'ont',\n",
       " 'Ġagain',\n",
       " 'clud',\n",
       " 'tern',\n",
       " 'Ġknow',\n",
       " 'Ġneed',\n",
       " 'Ġcons',\n",
       " 'Ġco',\n",
       " 'Ġ.',\n",
       " 'Ġwant',\n",
       " 'Ġsee',\n",
       " 'Ġ7',\n",
       " 'ning',\n",
       " 'iew',\n",
       " 'ĠThis',\n",
       " 'ced',\n",
       " 'Ġeven',\n",
       " 'Ġind',\n",
       " 'ty',\n",
       " 'ĠWe',\n",
       " 'ath',\n",
       " 'Ġthese',\n",
       " 'Ġpr',\n",
       " 'Ġuse',\n",
       " 'Ġbecause',\n",
       " 'Ġfl',\n",
       " 'ng',\n",
       " 'Ġnow',\n",
       " 'ĠâĢĵ',\n",
       " 'com',\n",
       " 'ise',\n",
       " 'Ġmake',\n",
       " 'Ġthen',\n",
       " 'ower',\n",
       " 'Ġevery',\n",
       " 'ĠUn',\n",
       " 'Ġsec',\n",
       " 'oss',\n",
       " 'uch',\n",
       " 'Ġem',\n",
       " 'Ġ=',\n",
       " 'ĠRe',\n",
       " 'ied',\n",
       " 'rit',\n",
       " 'Ġinv',\n",
       " 'lect',\n",
       " 'Ġsupp',\n",
       " 'ating',\n",
       " 'Ġlook',\n",
       " 'man',\n",
       " 'pect',\n",
       " 'Ġ8',\n",
       " 'row',\n",
       " 'Ġbu',\n",
       " 'Ġwhere',\n",
       " 'ific',\n",
       " 'Ġyears',\n",
       " 'ily',\n",
       " 'Ġdiff',\n",
       " 'Ġshould',\n",
       " 'Ġrem',\n",
       " 'Th',\n",
       " 'In',\n",
       " 'Ġev',\n",
       " 'day',\n",
       " \"'re\",\n",
       " 'rib',\n",
       " 'Ġrel',\n",
       " 'ss',\n",
       " 'Ġdef',\n",
       " 'Ġright',\n",
       " 'Ġsy',\n",
       " '),',\n",
       " 'les',\n",
       " '000',\n",
       " 'hen',\n",
       " 'Ġthrough',\n",
       " 'ĠTr',\n",
       " '__',\n",
       " 'Ġway',\n",
       " 'Ġdon',\n",
       " 'Ġ,',\n",
       " 'Ġ10',\n",
       " 'ased',\n",
       " 'Ġass',\n",
       " 'ublic',\n",
       " 'Ġreg',\n",
       " 'ĠAnd',\n",
       " 'ix',\n",
       " 'Ġvery',\n",
       " 'Ġinclud',\n",
       " 'other',\n",
       " 'Ġimp',\n",
       " 'oth',\n",
       " 'Ġsub',\n",
       " 'ĠâĢĶ',\n",
       " 'Ġbeing',\n",
       " 'arg',\n",
       " 'ĠWh',\n",
       " '==',\n",
       " 'ible',\n",
       " 'Ġdoes',\n",
       " 'ange',\n",
       " 'ram',\n",
       " 'Ġ9',\n",
       " 'ert',\n",
       " 'ps',\n",
       " 'ited',\n",
       " 'ational',\n",
       " 'Ġbr',\n",
       " 'Ġdown',\n",
       " 'Ġmany',\n",
       " 'aking',\n",
       " 'Ġcall',\n",
       " 'uring',\n",
       " 'ities',\n",
       " 'Ġph',\n",
       " 'ics',\n",
       " 'als',\n",
       " 'Ġdec',\n",
       " 'ative',\n",
       " 'ener',\n",
       " 'Ġbefore',\n",
       " 'ility',\n",
       " 'Ġwell',\n",
       " 'Ġmuch',\n",
       " 'erson',\n",
       " 'Ġthose',\n",
       " 'Ġsuch',\n",
       " 'Ġke',\n",
       " 'Ġend',\n",
       " 'ĠBut',\n",
       " 'ason',\n",
       " 'ting',\n",
       " 'Ġlong',\n",
       " 'ef',\n",
       " 'Ġthink',\n",
       " 'ys',\n",
       " 'Ġbel',\n",
       " 'Ġsm',\n",
       " 'its',\n",
       " 'ax',\n",
       " 'Ġown',\n",
       " 'Ġprov',\n",
       " 'Ġset',\n",
       " 'ife',\n",
       " 'ments',\n",
       " 'ble',\n",
       " 'ward',\n",
       " 'Ġshow',\n",
       " 'Ġpres',\n",
       " 'ms',\n",
       " 'omet',\n",
       " 'Ġob',\n",
       " 'Ġsay',\n",
       " 'ĠSh',\n",
       " 'ts',\n",
       " 'ful',\n",
       " 'Ġeff',\n",
       " 'Ġgu',\n",
       " 'Ġinst',\n",
       " 'und',\n",
       " 'ren',\n",
       " 'cess',\n",
       " 'Ġent',\n",
       " 'ĠYou',\n",
       " 'Ġgood',\n",
       " 'Ġstart',\n",
       " 'ince',\n",
       " 'Ġmade',\n",
       " 'tt',\n",
       " 'stem',\n",
       " 'olog',\n",
       " 'up',\n",
       " 'Ġ|',\n",
       " 'ump',\n",
       " 'Ġhel',\n",
       " 'vern',\n",
       " 'ular',\n",
       " 'ually',\n",
       " 'Ġac',\n",
       " 'Ġmon',\n",
       " 'Ġlast',\n",
       " 'Ġ200',\n",
       " '10',\n",
       " 'Ġstud',\n",
       " 'ures',\n",
       " 'ĠAr',\n",
       " 'self',\n",
       " 'ars',\n",
       " 'meric',\n",
       " 'ues',\n",
       " 'cy',\n",
       " 'Ġmin',\n",
       " 'ollow',\n",
       " 'Ġcol',\n",
       " 'io',\n",
       " 'Ġmod',\n",
       " 'Ġcount',\n",
       " 'ĠCom',\n",
       " 'hes',\n",
       " 'Ġfin',\n",
       " 'air',\n",
       " 'ier',\n",
       " 'âĢĶ',\n",
       " 'read',\n",
       " 'ank',\n",
       " 'atch',\n",
       " 'ever',\n",
       " 'Ġstr',\n",
       " 'Ġpoint',\n",
       " 'ork',\n",
       " 'ĠNew',\n",
       " 'Ġsur',\n",
       " 'ool',\n",
       " 'alk',\n",
       " 'ement',\n",
       " 'Ġused',\n",
       " 'ract',\n",
       " 'ween',\n",
       " 'Ġsame',\n",
       " 'oun',\n",
       " 'ĠAl',\n",
       " 'ci',\n",
       " 'Ġdiffere',\n",
       " 'Ġwhile',\n",
       " '--------',\n",
       " 'Ġgame',\n",
       " 'cept',\n",
       " 'Ġsim',\n",
       " '...',\n",
       " 'Ġinter',\n",
       " 'ek',\n",
       " 'Ġreport',\n",
       " 'Ġprodu',\n",
       " 'Ġstill',\n",
       " 'led',\n",
       " 'ah',\n",
       " 'Ġhere',\n",
       " 'Ġworld',\n",
       " 'Ġthough',\n",
       " 'Ġnum',\n",
       " 'arch',\n",
       " 'imes',\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text_train.token2ind.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcbe246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_emb = 1024\n",
    "\n",
    "params_emb = {\n",
    "    'sequence_length': max_len,\n",
    "    'vocab_size': len(text_train),\n",
    "    'batch_size': batch_size_emb,\n",
    "    'shuffle': True,\n",
    "    'embedding': True\n",
    "}\n",
    "\n",
    "train_generator_emb = TextDataGenerator(seq_train.sequences, seq_train.next_words, **params_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad092a56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mw:\\Documents\\School docs\\4th Year 1st sem\\Thesis 2\\jupyter\\BERT-BLSTM Model\\GPT_BLSTM\\Test.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/w%3A/Documents/School%20docs/4th%20Year%201st%20sem/Thesis%202/jupyter/BERT-BLSTM%20Model/GPT_BLSTM/Test.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtokenizers/chef_tokenizer_with_special_tokens\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/w%3A/Documents/School%20docs/4th%20Year%201st%20sem/Thesis%202/jupyter/BERT-BLSTM%20Model/GPT_BLSTM/Test.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GPT2Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36b50167",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "tokenizers/chef_tokenizer_with_special_tokens-vocab.json is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/tokenizers/chef_tokenizer_with_special_tokens-vocab.json/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\transformers\\utils\\hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    430\u001b[0m         path_or_repo_id,\n\u001b[0;32m    431\u001b[0m         filename,\n\u001b[0;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\huggingface_hub\\file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[0;32m   1196\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1197\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1198\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1199\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m   1200\u001b[0m     )\n\u001b[0;32m   1201\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[0;32m   1202\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\huggingface_hub\\file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1533\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1534\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m   1540\u001b[0m )\n\u001b[1;32m-> 1541\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[0;32m    286\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-65229cf1-1a8fb6734d2dc6dc2e9adc34;b8da315d-9c77-4b40-90fd-9ad073600789)\n\nRepository Not Found for url: https://huggingface.co/tokenizers/chef_tokenizer_with_special_tokens-vocab.json/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mw:\\Documents\\School docs\\4th Year 1st sem\\Thesis 2\\jupyter\\BERT-BLSTM Model\\GPT_BLSTM\\Test.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/w%3A/Documents/School%20docs/4th%20Year%201st%20sem/Thesis%202/jupyter/BERT-BLSTM%20Model/GPT_BLSTM/Test.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mconstants\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mc\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/w%3A/Documents/School%20docs/4th%20Year%201st%20sem/Thesis%202/jupyter/BERT-BLSTM%20Model/GPT_BLSTM/Test.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtokenizers/chef_tokenizer_with_special_tokens-vocab.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/w%3A/Documents/School%20docs/4th%20Year%201st%20sem/Thesis%202/jupyter/BERT-BLSTM%20Model/GPT_BLSTM/Test.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:686\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    685\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 686\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    687\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    688\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:519\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m     token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m    518\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 519\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    520\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    521\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[0;32m    522\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    523\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    524\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    525\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    526\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    527\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    528\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    529\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    530\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    531\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    532\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    533\u001b[0m )\n\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mw:\\anaconda3\\envs\\Viver\\lib\\site-packages\\transformers\\utils\\hub.py:450\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 450\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    451\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    457\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: tokenizers/chef_tokenizer_with_special_tokens-vocab.json is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer\n",
    "import constants as c\n",
    "model_name = 'tokenizers/chef_tokenizer_with_special_tokens-vocab.json'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# new_tokens = list(set(c.NEW_TOKENS) - set(tokenizer.get_vocab().keys()))\n",
    "# tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e05bf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27,\n",
       " 38827,\n",
       " 4061,\n",
       " 36,\n",
       " 62,\n",
       " 2257,\n",
       " 7227,\n",
       " 29,\n",
       " 1279,\n",
       " 21479,\n",
       " 62,\n",
       " 2257,\n",
       " 7227,\n",
       " 29,\n",
       " 1279,\n",
       " 49560,\n",
       " 2538,\n",
       " 62,\n",
       " 10619,\n",
       " 29,\n",
       " 2323,\n",
       " 12023]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# \"\".join(tokenizer.batch_decode(tokenizer.encode('<RECIPE_START> <NER_START> <TITLE_END> ground beef')))\n",
    "tokenizer.encode('<RECIPE_START> <NER_START> <TITLE_END> ground beef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55e76603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 30530, 1026, 11265, 2099, 1035, 2707, 1028, 2598, 12486, 102]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer.encode('<RECIPE_START> <NER_START> ground beef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ee3f7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' <NER_START> ',\n",
       " ' <INGREDIENTS_END>',\n",
       " 'brown',\n",
       " 's',\n",
       " 'ugar',\n",
       " '.',\n",
       " '<',\n",
       " 'IN',\n",
       " 'STRUCT',\n",
       " 'save',\n",
       " 'dash',\n",
       " 'of',\n",
       " 't',\n",
       " 'aste',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'B',\n",
       " 'ake',\n",
       " 'in',\n",
       " '1',\n",
       " '/',\n",
       " '3',\n",
       " 'stick',\n",
       " 'marg',\n",
       " 'arine',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'Mix',\n",
       " 'all',\n",
       " '.',\n",
       " '<',\n",
       " 'IN',\n",
       " 'STRUCT',\n",
       " 'ipped',\n",
       " 'min',\n",
       " 'utes',\n",
       " '.',\n",
       " '<',\n",
       " 'IN',\n",
       " 'STRUCT',\n",
       " 'ables',\n",
       " 'ug',\n",
       " 'ake',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'Roll',\n",
       " 'into',\n",
       " 'gre',\n",
       " 'ase',\n",
       " 'sh',\n",
       " 'illa',\n",
       " 'sk',\n",
       " 'im',\n",
       " 'tight',\n",
       " 'ipping',\n",
       " \"'\",\n",
       " 'S',\n",
       " 'aus',\n",
       " 'age',\n",
       " ' <TITLE_START> ',\n",
       " '<',\n",
       " 'REC',\n",
       " 'IP',\n",
       " 'oil',\n",
       " 'in',\n",
       " 'mic',\n",
       " 'row',\n",
       " 'ave',\n",
       " '-',\n",
       " 'or',\n",
       " '2',\n",
       " 'c',\n",
       " 'andy',\n",
       " 'and',\n",
       " 'm',\n",
       " 'ixture',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'Cut',\n",
       " 'into',\n",
       " 'c',\n",
       " 'abbage',\n",
       " 'or',\n",
       " 'dry',\n",
       " 'green',\n",
       " 'beans',\n",
       " ')',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'You',\n",
       " 'can',\n",
       " 'be',\n",
       " 'ust',\n",
       " 'on',\n",
       " 'top',\n",
       " '.',\n",
       " '<',\n",
       " 'IN',\n",
       " 'STRUCT',\n",
       " 'rots',\n",
       " 'and',\n",
       " 'nuts',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'B',\n",
       " 'ake',\n",
       " 'at',\n",
       " '350',\n",
       " '\\\\',\n",
       " 't',\n",
       " 'aste',\n",
       " ' <TITLE_START> ',\n",
       " ' <TITLE_START> ',\n",
       " '1',\n",
       " 'c',\n",
       " '.',\n",
       " 'mini',\n",
       " 'ature',\n",
       " 'm',\n",
       " 'ixture',\n",
       " ' <TITLE_START> ',\n",
       " '1',\n",
       " '/',\n",
       " '2',\n",
       " 'c',\n",
       " 'ups',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'St',\n",
       " 'ir',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ef',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'M',\n",
       " 'akes',\n",
       " '1',\n",
       " 'can',\n",
       " 'cher',\n",
       " 'ries',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'B',\n",
       " 'ake',\n",
       " 'at',\n",
       " '350',\n",
       " '\\\\',\n",
       " 'u',\n",
       " '\\\\',\n",
       " 't',\n",
       " 'rav',\n",
       " 'y',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'Roll',\n",
       " 'in',\n",
       " ' <TITLE_START> ',\n",
       " '1',\n",
       " 'ts',\n",
       " 'when',\n",
       " 'meat',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'Cover',\n",
       " 'with',\n",
       " 'a',\n",
       " 'gre',\n",
       " 'ase',\n",
       " 'ce',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'D',\n",
       " 'rain',\n",
       " 'green',\n",
       " 'beans',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'Cut',\n",
       " 'op',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'Add',\n",
       " 'd',\n",
       " 'ill',\n",
       " '.',\n",
       " '<',\n",
       " 'IN',\n",
       " 'STRUCT',\n",
       " 'os',\n",
       " '.',\n",
       " ' <TITLE_START> ',\n",
       " 'B',\n",
       " 'ake',\n",
       " 'in',\n",
       " 'the',\n",
       " 'still',\n",
       " 'Free',\n",
       " 'reens',\n",
       " '<',\n",
       " 'ING',\n",
       " 'RED',\n",
       " 'may',\n",
       " 'ju',\n",
       " 'ice']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([50261, 50268, 33282, 82, 35652, 13, 27, 1268, 46126, 21928, 42460, 1659, 83, 4594, 13, 50267, 33, 539, 259, 16, 14, 18, 13915, 30887, 34569, 13, 50267, 35608, 439, 13, 27, 1268, 46126, 3949, 1084, 1769, 13, 27, 1268, 46126, 2977, 1018, 539, 13, 50267, 26869, 20424, 16694, 589, 1477, 5049, 8135, 320, 33464, 4501, 6, 50, 8717, 496, 50267, 27, 38827, 4061, 9437, 259, 9383, 808, 1015, 12, 273, 17, 66, 10757, 392, 76, 9602, 13, 50267, 26254, 20424, 66, 32061, 273, 39140, 14809, 44749, 8, 13, 50267, 1639, 5171, 1350, 436, 261, 4852, 13, 27, 1268, 46126, 24744, 392, 31381, 13, 50267, 33, 539, 265, 14877, 59, 83, 4594, 50267, 50267, 16, 66, 13, 45313, 1300, 76, 9602, 50267, 16, 14, 17, 66, 4739, 13, 50267, 1273, 343, 259, 1169, 891, 13, 50267, 44, 1124, 16, 5171, 2044, 1678, 13, 50267, 33, 539, 265, 14877, 59, 84, 59, 83, 4108, 88, 13, 50267, 26869, 259, 50267, 16, 912, 12518, 41495, 13, 50267, 27245, 4480, 64, 16694, 589, 344, 13, 50267, 35, 3201, 14809, 44749, 13, 50267, 26254, 404, 13, 50267, 4550, 67, 359, 13, 27, 1268, 46126, 418, 13, 50267, 33, 539, 259, 1169, 24219, 11146, 5681, 27, 2751, 22083, 11261, 14396, 501])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
